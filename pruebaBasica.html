<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prueba de Vida</title>
    <script defer src="face-api.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin: 0;
            padding: 0;
        }

        video {
            border: 1px solid #ccc;
            width: 400px;
            height: auto;
            margin-top: 20px;
        }

        #status {
            margin-top: 10px;
            font-size: 1.2em;
            color: #333;
        }

        canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>

<body>
    <h1>Prueba de Vida</h1>
    <video id="video" autoplay muted></video>
    <p id="status">Cargando modelos...</p>
    <script>
        // Inicia el proceso de detección
        async function startLifeDetection() {
            const video = document.getElementById('video');
            const status = document.getElementById('status');

            // Cargar modelos de Face API
            await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
            await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
            await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
            await faceapi.nets.faceExpressionNet.loadFromUri('/models');

            status.textContent = 'Solicitando acceso a la cámara...';

            // Procesar video para la detección facial
            video.addEventListener('play', () => {
                const canvas = faceapi.createCanvasFromMedia(video);
                document.body.append(canvas);

                const displaySize = { width: video.width, height: video.height };
                faceapi.matchDimensions(canvas, displaySize);

                let checks = 0;
                let isAlive = false;

                setInterval(async () => {
                    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                        .withFaceLandmarks();

                    if (detections.length > 0) {
                        status.textContent = 'Rostro detectado. Realizando pruebas...';

                        const landmarks = detections[0].landmarks;
                        const leftEye = landmarks.getLeftEye();
                        const rightEye = landmarks.getRightEye();

                        if (isEyeClosed(leftEye) || isEyeClosed(rightEye)) {
                            isAlive = true;
                        }

                        checks++;

                        if (checks >= 5 && isAlive) {
                            status.textContent = 'Prueba de vida superada.';
                            clearInterval(this);
                        }
                    } else {
                        status.textContent = 'Por favor, muestre su rostro.';
                    }

                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
                }, 500);
            });

            function isEyeClosed(eye) {
                const verticalDist = Math.abs(eye[1].y - eye[5].y);
                const horizontalDist = Math.abs(eye[0].x - eye[3].x);
                return verticalDist / horizontalDist < 0.2; // Ajusta según necesidad
            }
        }

        function startVideo() {
            navigator.getUserMedia(
                { video: {} },
                stream => video.srcObject = stream,
                err => console.error(err)
            );
        }

        // Iniciar el proceso
        startVideo();
        startLifeDetection();
    </script>
</body>

</html>